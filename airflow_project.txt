AIRFLOW
ASTRO
DBEAVER
DOCKER
DBEAVER
POSTGRES
WEATHER API






1) create a Astro acnt
   *install Astro cli (command line interface)
   *docker acnt
   *enable Microsoft Hyper-V 
      PROCEDURE
       1)control panel
       2) turn windowa features on or off (on the left)
       3)enable Hyper-V  ***********remember to off it after use
   * Astro installation command - https://www.astronomer.io/docs/astro/cli/install-cli
      1) ************************uninstall Astro CLI after use 
         https://www.astronomer.io/docs/astro/cli/install-cli?tab=windowswithwinget#upgrade-the-cli

                        or
   *install Astro cli using command "winget install -e --id Astronomer.Astro" in vs code
2)Start a airflow project 
  *in vscode 
          astro dev init
  *it will give all the reuired folders

3)create etlweather.py file
***import dag-
***http hook -to extract data fro api
***postreges hook - to push or load data to postresg database 
                  (there are dif types of hooks


1. Database Hooks
PostgresHook
MySqlHook
SQLiteHook
OracleHook

2. Cloud Provider Hooks
S3Hook: 
GCSHook: 
AzureBlobStorageHook: 
GoogleCloudStorageHook:

3. Messaging Hooks
SqsHook: 
PubSubHook:

4. HTTP and API Hooks
HttpHook: 
SlackAPIPostOperator: 


5. Data Processing Hooks
SparkSubmitHook: 
HiveHook: 


6. Machine Learning Hooks
KubernetesPodOperator: 


7. File System Hooks
FileSensor: 
FtpHook:

8. Custom Hooks
You can create your own hooks by extending airflow.hooks.base_hook.BaseHook to interact with specific systems or APIs not covered by existing hooks.



                                 //////////////// IMPORTING LIB /////////////////////
from airflow import DAG
from airflow.providers.https.hooks.http import HttpHook
from airflow.providers.postgres.hooks.postgres import PostgresHook

from airflow.decorators import task  
from airflow.utils.dates import days_ago

           //////////////////////////////////////////////////////////////


business case - when you enter the latidtude and longitude of specific location it should give all the information regarding like climate , templerature etc.
we can extract it using weather app api


                  ///////////////////main code////////////////////////

1) Import library
2) define logitute and latitude
3) give connection id for postgres and api
4) provide default arguments 
5)create dag
          1 provide - DAG Name (dag_id="")
          2 default_args
          3 schedule_interval - it should run daily
          4 catchup
6) inside DAG define task
    *there are 3 task extract, transform, load


    6.1)EXTACT

         ////  what we are doing here *Extract weather data from Open-Meteo API using Airflow Connection///


       *Use HTTP Hook to get connection details from Airflow connection
       *intialise http hook connection
         1) hook connection id
         2)method ="get"
       *Build the API endpoint
       *hit the end point
       *if else condition- if you get the data then store it in json format orlse show error message
         

      6.2) Transform
           *Transform the extracted weather data.
    
      6.3)Load data
           *load data to postgres db - define postgres hook, conn, coursor
           *create a table if not exist
           *then load the transformed data into the table


7)DAG Worflow- ETL Pipeline



1)
from airflow import DAG
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.decorators import task
from airflow.utils.dates import days_ago
import requests
import json

2)
LATITUDE = '51.5074'
LONGITUDE = '-0.1278'

3)
POSTGRES_CONN_ID='postgres_default'
API_CONN_ID='open_meteo_api'

4)
default_args={
    'owner':'airflow',
    'start_date':days_ago(1)
}

5)
with DAG(dag_id='weather_etl_pipeline',
         default_args=default_args,
         schedule_interval='@daily',
         catchup=False) as dags:

6)
6.1
@task()
    def extract_weather_data():
        """Extract weather data from Open-Meteo API using Airflow Connection."""

        # Use HTTP Hook to get connection details from Airflow connection

        http_hook=HttpHook(http_conn_id=API_CONN_ID,method='GET')

        ## Build the API endpoint
        ## https://api.open-meteo.com/v1/forecast?latitude=51.5074&longitude=-0.1278&current_weather=true
        endpoint=f'/v1/forecast?latitude={LATITUDE}&longitude={LONGITUDE}&current_weather=true'

        ## Make the request via the HTTP Hook
        response=http_hook.run(endpoint)

        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(f"Failed to fetch weather data: {response.status_code}")


6.2

    @task()
    def transform_weather_data(weather_data):
        """Transform the extracted weather data."""
        current_weather = weather_data['current_weather']
        transformed_data = {
            'latitude': LATITUDE,
            'longitude': LONGITUDE,
            'temperature': current_weather['temperature'],
            'windspeed': current_weather['windspeed'],
            'winddirection': current_weather['winddirection'],
            'weathercode': current_weather['weathercode']
        }
        return transformed_data
    

6.3


    @task()
    def load_weather_data(transformed_data):
        """Load transformed data into PostgreSQL."""
        pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
        conn = pg_hook.get_conn()
        cursor = conn.cursor()

        # Create table if it doesn't exist
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS weather_data (
            latitude FLOAT,
            longitude FLOAT,
            temperature FLOAT,
            windspeed FLOAT,
            winddirection FLOAT,
            weathercode INT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """)

        # Insert transformed data into the table
        cursor.execute("""
        INSERT INTO weather_data (latitude, longitude, temperature, windspeed, winddirection, weathercode)
        VALUES (%s, %s, %s, %s, %s, %s)
        """, (
            transformed_data['latitude'],
            transformed_data['longitude'],
            transformed_data['temperature'],
            transformed_data['windspeed'],
            transformed_data['winddirection'],
            transformed_data['weathercode']
        ))

        conn.commit()
        cursor.close()
 7)

 weather_data= extract_weather_data()
    transformed_data=transform_weather_data(weather_data)
    load_weather_data(transformed_data)






////////////////////after completeing the code//////////////////////

3)create docker-compose,yaml file

*to run postgres sql in docker


*volumes:
      - postgres_data:/var/lib/postgresql/data

*even if you restart the data should not be earsed.
 so it will be saved int the loaction-var/lib/postgresql/data

*after creating docker file - open docker then run these command
$docker-compose up
***********************after use please run the command
$docker-compose down
This will stop and remove the containers defined in your docker-compose.yml.


4)Run the entire project - in another powershell
*$astro dev start

5)airflow interface opens - set the connection

connect - postgres
connect = api


6) run the dag


click on graph-> click transform dag-> here you can click xcom ->transformed data can be seen. 
click ->load->xcom-> there wont be any data because data is loaded into postgress


7)
how to test the data is stored in postgres or not
*we are running our postgres in docker, so there you can see the port num. but when you click it , you cant see anything, because everything is runningin docker

*to see actual whether it is loading the data to postgres data base or not . we van use dbeaver.

dbeaver->open sourcw-> helps to connect with any database we want

*database->connect to a database

*hostname->username->password-> (as given in airflow connection)test connection->download the file->succesfull

*check whether the table created in present or not 
postgres->database->postgres-> schmema-> public->tables
see whether the table is present 

*how to write query
SQL EDITOR->open sql script-> select * from weather_data

you can see the records there
*now here you can run the pipeline once in airflow then see the dbeaver -> you can see the database updated with another record

live data getting updated


IF YOU WANT TO LOAD THIS DATA TO AMAZON RDS(deployment)->create postgressql in amazon rds -> take the endpoint (copy it) ->then paste it in the host(initial it was the name from docker but now from cloud) of airflow connection(postgres connection)

data gets loaded into cloud










